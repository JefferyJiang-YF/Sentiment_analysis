{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import package"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T09:09:34.081650Z","iopub.status.busy":"2024-04-05T09:09:34.081282Z","iopub.status.idle":"2024-04-05T09:09:41.405272Z","shell.execute_reply":"2024-04-05T09:09:41.404492Z","shell.execute_reply.started":"2024-04-05T09:09:34.081618Z"},"trusted":true},"outputs":[],"source":["import collections\n","\n","import datasets \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchtext\n","import tqdm\n","import transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:26:45.761737Z","iopub.status.busy":"2024-03-24T16:26:45.760805Z","iopub.status.idle":"2024-03-24T16:26:45.770946Z","shell.execute_reply":"2024-03-24T16:26:45.770033Z","shell.execute_reply.started":"2024-03-24T16:26:45.761709Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# !pip install numpy==1.23.5\n","# !pip install sacremoses\n","# !pip install evaluate\n","# Restart your kernel\n","\n","seed = 9072\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["# You should login the HF in order to upload your model"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:26:45.772335Z","iopub.status.busy":"2024-03-24T16:26:45.771997Z","iopub.status.idle":"2024-03-24T16:26:45.797684Z","shell.execute_reply":"2024-03-24T16:26:45.796852Z","shell.execute_reply.started":"2024-03-24T16:26:45.772302Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a85b23494de0443d87166cb6ad7a545d","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:26:51.037483Z","iopub.status.busy":"2024-03-24T16:26:51.036707Z","iopub.status.idle":"2024-03-24T16:27:41.714189Z","shell.execute_reply":"2024-03-24T16:27:41.713338Z","shell.execute_reply.started":"2024-03-24T16:26:51.037445Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"458dacf7a5fc42c19798b42cd7888cf1","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79eb29b8a1ac4b69b90e800130c45c4b","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5893a906881e4ad8984b7525953dfb15","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33dfca8c8050473ead433212b89184cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset, load_metric\n","\n","imdb = load_dataset(\"imdb\")\n","\n","###\n","# from datasets import load_dataset\n","# imdb_train = load_dataset('imdb',split=\"train\")\n","# imdb_test = load_dataset('imdb',split=\"test[:6250]+test[-6250:]\")\n","# imdb_val = load_dataset('imdb',split=\"test[6250:12500]+test[-12500:-6250]\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T13:21:30.145747Z","iopub.status.busy":"2024-03-23T13:21:30.144858Z","iopub.status.idle":"2024-03-23T13:21:30.152306Z","shell.execute_reply":"2024-03-23T13:21:30.151430Z","shell.execute_reply.started":"2024-03-23T13:21:30.145714Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["imdb"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T09:13:12.447905Z","iopub.status.busy":"2024-04-05T09:13:12.447223Z","iopub.status.idle":"2024-04-05T09:13:17.733629Z","shell.execute_reply":"2024-04-05T09:13:17.732684Z","shell.execute_reply.started":"2024-04-05T09:13:12.447862Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ad93c5611e48d185519f363789119e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd5b93d219924322b0cf67e0c365853e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"847454e162f04b95988ebfe07ce6b1fc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20cd45dec38845a2a0711033261491b4","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74e2122bee294578ba7e3936022e7ff7","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["The model albert-base-v2 has 11685122 trainable parameters.\n"]}],"source":["# Import the AutoModelForSequenceClassification and AutoTokenizer classes from the transformers library.\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# Define possible model names for later use, with the current one set to 'albert-base-v2'.\n","# model_name = \"bert-base-uncased\"        # Commented out alternative model name for BERT base model\n","# model_name = \"distilbert-base-uncased\"  # Commented out alternative model name for DistilBERT base model\n","model_name = \"albert-base-v2\"             # The selected model name to be used\n","\n","# Load the pre-trained ALBERT model for sequence classification with custom label mappings for binary classification.\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    id2label={0: \"Negative\", 1: \"Positive\"},\n","    label2id={\"Negative\": 0, \"Positive\": 1}\n",")\n","\n","# Load the tokenizer that corresponds to the ALBERT model.\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Re-import AutoModelForSequenceClassification and AutoTokenizer for further use in the function.\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# Define a function that estimates the number of trainable parameters for a given model name.\n","def estimate_parameters(model_name):\n","    # Load the model with the specified name and custom label mappings.\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        id2label={0: \"Negative\", 1: \"Positive\"},\n","        label2id={\"Negative\": 0, \"Positive\": 1}\n","    )\n","    \n","    # Calculate the total number of trainable parameters in the model.\n","    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","    # Return the calculated number of parameters.\n","    return num_parameters\n","\n","# Example usage of the function: compute the number of trainable parameters for the selected model.\n","num_params = estimate_parameters(model_name)\n","\n","# Print a statement with the result that shows the number of trainable parameters of the selected model.\n","print(f\"The model {model_name} has {num_params} trainable parameters.\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:27:57.166008Z","iopub.status.busy":"2024-03-24T16:27:57.165545Z","iopub.status.idle":"2024-03-24T16:28:52.421452Z","shell.execute_reply":"2024-03-24T16:28:52.420308Z","shell.execute_reply.started":"2024-03-24T16:27:57.165983Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"604ca942db27458fbb4cc0533983c7c9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00c0ccd75325419fa0e858c1410c1238","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84959c1d75ae48cf8023b9a0cd3f517c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Define a function that tokenizes the 'text' field in a dataset example.\n","def tokenize(examples):\n","    # Tokenize the input text and enable truncation to handle texts longer than the model's maximum input length.\n","    outputs = tokenizer(examples['text'], truncation=True)\n","    # Return the tokenized outputs.\n","    return outputs\n","\n","# Apply the 'tokenize' function to the entire imdb dataset, handling the examples in batches for efficiency.\n","tokenized_ds = imdb.map(tokenize, batched=True)\n","\n","# The following commented section shows how to tokenize different splits of a dataset (train, test, validation)\n","# in a batched fashion, with padding and truncation enabled, and with a specified batch size.\n","###\n","# Tokenize the training set, applying padding and truncation, processing in batches of size 1000.\n","# enc_train = imdb_train.map(lambda e: tokenizer(e['text'],padding=True,truncation=True),batched=True,batch_size=1000)\n","\n","# Tokenize the testing set in the same way as the training set.\n","# enc_test = imdb_test.map(lambda e: tokenizer(e['text'],padding=True,truncation=True),batched=True,batch_size=1000)\n","\n","# Tokenize the validation set in the same way as the training and testing sets.\n","# enc_val = imdb_val.map(lambda e: tokenizer(e['text'],padding=True,truncation=True),batched=True,batch_size=1000)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T13:46:34.623081Z","iopub.status.busy":"2024-03-23T13:46:34.622486Z","iopub.status.idle":"2024-03-23T13:46:35.154156Z","shell.execute_reply":"2024-03-23T13:46:35.152982Z","shell.execute_reply.started":"2024-03-23T13:46:34.623043Z"},"trusted":true},"outputs":[],"source":["# import numpy as np\n","\n","# def compute_metrics(eval_preds):\n","#     metric = load_metric(\"accuracy\")\n","#     logits, labels = eval_preds\n","#     predictions = np.argmax(logits, axis=-1)\n","#     return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:28:52.424307Z","iopub.status.busy":"2024-03-24T16:28:52.423605Z","iopub.status.idle":"2024-03-24T16:29:02.619005Z","shell.execute_reply":"2024-03-24T16:29:02.618046Z","shell.execute_reply.started":"2024-03-24T16:28:52.424270Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-24 16:28:55.039801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-24 16:28:55.039912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-24 16:28:55.171090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n","data_collator = DataCollatorWithPadding(tokenizer) "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:29:46.990238Z","iopub.status.busy":"2024-03-24T16:29:46.989870Z","iopub.status.idle":"2024-03-24T16:29:46.998709Z","shell.execute_reply":"2024-03-24T16:29:46.997842Z","shell.execute_reply.started":"2024-03-24T16:29:46.990210Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(num_train_epochs=2,\n","#                                 output_dir=\"distilbert-imdb\",\n","                                output_dir=\"bert-imdb\",\n","                                # output_dir=\"albert-imdb\",\n","                                push_to_hub=True,\n","                                per_device_train_batch_size=16,\n","                                per_device_eval_batch_size=16,\n","                                evaluation_strategy=\"epoch\",\n","                                seed=seed\n","                                 )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:29:51.340532Z","iopub.status.busy":"2024-03-24T16:29:51.340167Z","iopub.status.idle":"2024-03-24T16:29:51.346533Z","shell.execute_reply":"2024-03-24T16:29:51.345553Z","shell.execute_reply.started":"2024-03-24T16:29:51.340501Z"},"trusted":true},"outputs":[],"source":["# Import accuracy_score and precision_recall_fscore_support functions from sklearn.metrics for evaluation.\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# Define a function that computes evaluation metrics from predictions.\n","def compute_metrics(pred):\n","    # Extract ground truth labels from the predictions object.\n","    labels = pred.label_ids\n","    # Extract the predicted labels by taking the argmax of the logits from the predictions object.\n","    preds = pred.predictions.argmax(-1)\n","    # Compute precision, recall, and F1-score using macro averaging.\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    # Compute the accuracy score.\n","    acc = accuracy_score(labels, preds)\n","    # Return a dictionary with accuracy, F1, precision, and recall.\n","    return {\n","        'Accuracy': acc,\n","        'F1': f1,\n","        'Precision': precision,\n","        'Recall': recall\n","    }\n","\n","# The compute_metrics function is usually used as a parameter for the Trainer in Hugging Face's Transformers library,\n","# to compute evaluation metrics after each training epoch or after model evaluation."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:29:54.753385Z","iopub.status.busy":"2024-03-24T16:29:54.753001Z","iopub.status.idle":"2024-03-24T16:29:55.848037Z","shell.execute_reply":"2024-03-24T16:29:55.847277Z","shell.execute_reply.started":"2024-03-24T16:29:54.753353Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["trainer = Trainer(model=model, tokenizer=tokenizer,\n","                  data_collator=data_collator,\n","                  args=training_args,\n","#                   train_dataset=enc_train,\n","#                   eval_dataset=enc_val, \n","                  train_dataset=tokenized_ds['train'],\n","                  eval_dataset=tokenized_ds['test'],\n","                  compute_metrics=compute_metrics)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:29:57.086293Z","iopub.status.busy":"2024-03-24T16:29:57.085576Z","iopub.status.idle":"2024-03-24T16:29:57.090314Z","shell.execute_reply":"2024-03-24T16:29:57.089284Z","shell.execute_reply.started":"2024-03-24T16:29:57.086260Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:29:58.194365Z","iopub.status.busy":"2024-03-24T16:29:58.193550Z","iopub.status.idle":"2024-03-24T17:29:30.331466Z","shell.execute_reply":"2024-03-24T17:29:30.330543Z","shell.execute_reply.started":"2024-03-24T16:29:58.194329Z"},"trusted":true},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3126/3126 58:56, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.222300</td>\n","      <td>0.189848</td>\n","      <td>0.932760</td>\n","      <td>0.932747</td>\n","      <td>0.933098</td>\n","      <td>0.932760</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.116100</td>\n","      <td>0.226617</td>\n","      <td>0.939560</td>\n","      <td>0.939554</td>\n","      <td>0.939743</td>\n","      <td>0.939560</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=3126, training_loss=0.19396053974398114, metrics={'train_runtime': 3571.813, 'train_samples_per_second': 13.998, 'train_steps_per_second': 0.875, 'total_flos': 1.301775246386544e+16, 'train_loss': 0.19396053974398114, 'epoch': 2.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Save model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T17:30:17.203664Z","iopub.status.busy":"2024-03-24T17:30:17.202981Z","iopub.status.idle":"2024-03-24T17:30:56.995498Z","shell.execute_reply":"2024-03-24T17:30:56.994261Z","shell.execute_reply.started":"2024-03-24T17:30:17.203628Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34f168f513aa4616a23203b6d2ebfddd","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe888835fdda4f049f22bf5ab3bc0c7e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e11795c51a444ff4a99ec07b71b8df81","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1711297798.fef996fca6ad.34.0:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["('MyBestIMDBModel_Bert/tokenizer_config.json',\n"," 'MyBestIMDBModel_Bert/special_tokens_map.json',\n"," 'MyBestIMDBModel_Bert/vocab.txt',\n"," 'MyBestIMDBModel_Bert/added_tokens.json',\n"," 'MyBestIMDBModel_Bert/tokenizer.json')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model_save_path = \"MyBestIMDBModel_Bert\"\n","trainer.save_model(model_save_path)\n","tokenizer.save_pretrained(model_save_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Test your model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from transformers import AlbertTokenizerFast, AlbertForSequenceClassification, pipeline\n","\n","# model = AlbertForSequenceClassification.from_pretrained(\"/kaggle/working/MyBestIMDBModel\")\n","# tokenizer = AlbertTokenizerFast.from_pretrained(\"/kaggle/working/MyBestIMDBModel\")\n","# nlp = pipeline(\"sentiment-analysis\", model=model,tokenizer=tokenizer)\n","# print(nlp(\"the movie was very impressive\"))\n","# print(nlp(\"the text of the picture was very poor\"))\n","\n","from transformers import pipeline, DistilBertForSequenceClassification, DistilBertTokenizerFast\n","model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/MyBestIMDBModel_distiBert\")\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"/kaggle/working/MyBestIMDBModel_distilBert\")\n","nlp = pipeline(\"sentiment-analysis\", model=model,tokenizer=tokenizer)\n","print(nlp(\"the movie was very impressive\"))\n","print(nlp(\"the text of the picture was very poor\"))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T17:31:58.631722Z","iopub.status.busy":"2024-03-24T17:31:58.631046Z","iopub.status.idle":"2024-03-24T17:32:23.129627Z","shell.execute_reply":"2024-03-24T17:32:23.128652Z","shell.execute_reply.started":"2024-03-24T17:31:58.631691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["打包完成\n","2024-03-24 17:32:23.125894\n"]}],"source":["# Import necessary modules: os for interacting with the file system,\n","# zipfile for creating zip archives, and datetime for timestamping.\n","import os\n","import zipfile\n","import datetime\n","\n","# Define a function to compress a directory (packagePath) into a zip file (zipPath).\n","def file2zip(packagePath, zipPath):\n","    '''\n","  :param packagePath: The path to the directory that you want to compress.\n","  :param zipPath: The path where the resulting zip file will be saved.\n","  :return: None\n","  '''\n","    # Create a ZipFile object in write mode.\n","    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n","    # Walk through the directory structure.\n","    for path, dirNames, fileNames in os.walk(packagePath):\n","        # Get the relative file path to maintain directory structure in the zip.\n","        fpath = path.replace(packagePath, '')\n","        # Add all files in the current directory to the zip file.\n","        for name in fileNames:\n","            # Get the full path to the current file.\n","            fullName = os.path.join(path, name)\n","            # Construct the name to be used within the zip file.\n","            name = fpath + '\\\\' + name\n","            # Write the file under the constructed name to the zip file.\n","            zip.write(fullName, name)\n","    # Close the ZipFile object to finalize the zip file.\n","    zip.close()\n","\n","# This conditional block checks if the script is the main program and not an imported module.\n","if __name__ == \"__main__\":\n","    # Define the path to the directory you want to compress.\n","    packagePath = '/kaggle/working/MyBestIMDBModel_Bert'\n","    # Define the output path for the resulting zip file.\n","    zipPath = '/kaggle/working/MyBestIMDBModel_Bert.zip'\n","    # Check if a zip file with the same name already exists and remove it if it does.\n","    if os.path.exists(zipPath):\n","        os.remove(zipPath)\n","    # Call the file2zip function to compress the directory into a zip file.\n","    file2zip(packagePath, zipPath)\n","    # Print a confirmation that the packaging is complete.\n","    print(\"Packaging complete\")\n","    # Print the current UTC time.\n","    print(datetime.datetime.utcnow())"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T17:32:37.654524Z","iopub.status.busy":"2024-03-24T17:32:37.654155Z","iopub.status.idle":"2024-03-24T17:32:42.999307Z","shell.execute_reply":"2024-03-24T17:32:42.998044Z","shell.execute_reply.started":"2024-03-24T17:32:37.654494Z"},"trusted":true},"outputs":[{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/JeffreyJIANG/bert-imdb/commit/0b5f6f12a72ff34e210ee05f42369271d3ecf03b', commit_message='End of training', commit_description='', oid='0b5f6f12a72ff34e210ee05f42369271d3ecf03b', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","metadata":{},"source":["# Utilizing BERT for Text Classification with GPU Acceleration"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T04:57:49.042146Z","iopub.status.busy":"2024-04-06T04:57:49.041432Z","iopub.status.idle":"2024-04-06T04:57:49.654828Z","shell.execute_reply":"2024-04-06T04:57:49.653855Z","shell.execute_reply.started":"2024-04-06T04:57:49.042112Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Import the pipeline function from transformers, \n","# which provides a high-level API for various tasks including text classification.\n","from transformers import pipeline\n","# Import the torch library to work with PyTorch and utilize GPU capabilities.\n","import torch\n","\n","# Set the device to the first GPU available for running the model.\n","device = torch.device(\"cuda:0\")\n","# Create a text classification pipeline with a pre-trained model hosted on Hugging Face.\n","pipe = pipeline(\"text-classification\", model=\"JeffreyJIANG/distilbert-imdb\")\n","\n","# Import AutoTokenizer and AutoModelForSequenceClassification classes from transformers.\n","# These classes are used for tokenizing the inputs and loading the model for sequence classification tasks.\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Load the tokenizer for the pre-trained model \"JeffreyJIANG/distilbert-imdb\" from Hugging Face.\n","tokenizer = AutoTokenizer.from_pretrained(\"JeffreyJIANG/distilbert-imdb\")\n","# Load the model for sequence classification from the pre-trained model \"JeffreyJIANG/distilbert-imdb\".\n","model = AutoModelForSequenceClassification.from_pretrained(\"JeffreyJIANG/distilbert-imdb\")\n","# Move the model to the GPU device defined earlier for faster computation.\n","model.to(device)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:05:14.681228Z","iopub.status.busy":"2024-04-06T05:05:14.680587Z","iopub.status.idle":"2024-04-06T05:05:26.119343Z","shell.execute_reply":"2024-04-06T05:05:26.118391Z","shell.execute_reply.started":"2024-04-06T05:05:14.681193Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4e5df6c176c442aa3941d5b577d7179","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10ad02713faf4cec8a505180eb12c158","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24ba841f34e94158a3212d0165ea5b34","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d14ad3c955644b4bbddec20f685a89b5","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a33a315dafc94062ac9007362b92a159","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c72ec64f56064540a52dcb755fcb87bd","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","import torch\n","device = torch.device(\"cuda:0\")\n","\n","pipe = pipeline(\"text-classification\", model=\"JeffreyJIANG/bert-imdb\")\n","# Load model directly\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"JeffreyJIANG/bert-imdb\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"JeffreyJIANG/bert-imdb\")\n","model.to(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:07:43.385658Z","iopub.status.busy":"2024-04-06T05:07:43.384972Z","iopub.status.idle":"2024-04-06T05:07:46.855358Z","shell.execute_reply":"2024-04-06T05:07:46.854397Z","shell.execute_reply.started":"2024-04-06T05:07:43.385621Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9483057a34a342c7bebd2e4196698aa0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2d74785b2de4ad28b4a5dc5142475c8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be4ec4731f7a474098823c7a6857217a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f3c6e54e1c04b25a8cb82996bedde6a","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a9e85843c4f4996957eab10456e5c01","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.27M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"334ff6f92c894d93856feb287232e4d1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(512, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","              (activation): NewGELUActivation()\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=768, out_features=768, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","import torch\n","device = torch.device(\"cuda:0\")\n","\n","pipe = pipeline(\"text-classification\", model=\"JeffreyJIANG/albert-imdb\")\n","# Load model directly\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"JeffreyJIANG/albert-imdb\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"JeffreyJIANG/albert-imdb\")\n","model.to(device)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:07:51.245608Z","iopub.status.busy":"2024-04-06T05:07:51.245245Z","iopub.status.idle":"2024-04-06T05:07:51.253992Z","shell.execute_reply":"2024-04-06T05:07:51.253201Z","shell.execute_reply.started":"2024-04-06T05:07:51.245580Z"},"trusted":true},"outputs":[],"source":["def predict_sentiment(text, model, tokenizer, device):\n","    # Tokenize the input text and get the input IDs required by the BERT model.\n","    ids = tokenizer(text)[\"input_ids\"]\n","    # Convert the list of input IDs into a PyTorch tensor and add an extra dimension for batch size.\n","    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n","    # Disable gradient calculation as it is not needed during inference, which saves memory and computations.\n","    with torch.no_grad():\n","        # Pass the input tensor to the model to obtain the raw output, which includes the logits.\n","        outputs = model(tensor)\n","    # Extract the logits (unnormalized predictions) from the model output.\n","    logits = outputs.logits\n","    # Apply softmax function to the logits to get the probabilities of the classes.\n","    probability = torch.softmax(logits, dim=-1)\n","    # Determine the predicted class by finding the index of the highest logit value.\n","    predicted_class = logits.argmax(dim=-1).item()\n","    # Return the predicted class and the probabilities of all classes.\n","    return predicted_class, probability"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:07:53.536380Z","iopub.status.busy":"2024-04-06T05:07:53.535707Z","iopub.status.idle":"2024-04-06T05:07:53.541655Z","shell.execute_reply":"2024-04-06T05:07:53.540571Z","shell.execute_reply.started":"2024-04-06T05:07:53.536346Z"},"trusted":true},"outputs":[],"source":["# Case 1\n","text = (\n","    \"When thinking of boxing films, there is already a series of predecessors: \\\"Rocky\\\" (1976), \"\n","    \"\\\"Million Dollar Baby\\\" (2004), \\\"Unbeatable\\\" (2013), and so on. The movie \\\"Hot and Heavy\\\" \"\n","    \"adapted from Masaharu Take's \\\"100 Yen Love\\\" (2014, starring Sakura Ando) also tells a \"\n","    \"story of an underdog's rise and struggle, showcasing the passion for life encapsulated in the \"\n","    \"phrase \\\"You Only Live Once.\\\"\\n\\n\"\n","    \"The film addresses social phenomena in China such as \\\"lying flat\\\" (tang ping), otaku culture, \"\n","    \"the \\\"boomerang generation\\\" (those who depend on their parents), and the media's fabrication \"\n","    \"of reality, with the story set against the backdrop of card gambling. Although the movie was \"\n","    \"shot in the Guangdong region, it lacks a distinct Cantonese flavor and sense of place, \"\n","    \"resembling a typical second or third-tier city in China aimed at a nationwide audience. \"\n","    \"Surprisingly, Lee Kwok Lun, known for his role as Jiumozhi in the beloved TV drama \"\n","    \"\\\"Demi-Gods and Semi-Devils,\\\" represents a vestige of value from Hong Kong's film and \"\n","    \"television culture.\\n\\n\"\n","    \"The focus of the film is on the actress Jia Ling's remarkable physical transformation, \"\n","    \"where she gained approximately 50 pounds and then lost around 130 pounds. The process is \"\n","    \"deeply moving and demonstrates that individuals who have dreams and persist in pursuing them \"\n","    \"are the most beautiful. On another level, the film also shows Jia Ling's discipline over her \"\n","    \"body, reflecting both collective political elements and the pursuit of personal breakthroughs.\"\n",")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:43:45.278239Z","iopub.status.busy":"2024-04-06T05:43:45.277843Z","iopub.status.idle":"2024-04-06T05:43:45.283042Z","shell.execute_reply":"2024-04-06T05:43:45.281795Z","shell.execute_reply.started":"2024-04-06T05:43:45.278211Z"},"trusted":true},"outputs":[],"source":["# Case 2\n","text = \"This film is not terrible, it's great!\""]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:08:18.098378Z","iopub.status.busy":"2024-04-06T05:08:18.097758Z","iopub.status.idle":"2024-04-06T05:08:18.102393Z","shell.execute_reply":"2024-04-06T05:08:18.101469Z","shell.execute_reply.started":"2024-04-06T05:08:18.098347Z"},"trusted":true},"outputs":[],"source":["# Case 3\n","text = \"This film is not great, it's terrible!\""]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:08:35.231128Z","iopub.status.busy":"2024-04-06T05:08:35.230488Z","iopub.status.idle":"2024-04-06T05:08:35.236667Z","shell.execute_reply":"2024-04-06T05:08:35.235320Z","shell.execute_reply.started":"2024-04-06T05:08:35.231099Z"},"trusted":true},"outputs":[],"source":["# Case 4\n","text = (\n","    \"A film adaptation of a true story about Britain's \\\"Shtetl's List\\\". When Prime Minister Chamberlain's \"\n","    \"appeasement policy fuels Hitler's totalitarian ambitions, the Czech Republic is in dire straits. Nicholas \"\n","    \"Winton sees the situation deteriorating and, out of compassion, goes all out to save the Jewish children. \"\n","    \"The rabbi tells Winton not to leave things unfinished, and Winton's diligence in doing a good deed deserves \"\n","    \"to be remembered for generations to come, but at the most important moment, things don't turn out well, and \"\n","    \"there is no way back, leaving Winton with a lifetime of guilt. The unique experience of an individual can be \"\n","    \"a heavy lesson in history, worthy of rethinking by future generations.\\n\\n\"\n","    \"The Miracle Train hovers between the extraordinary period of 1938 and the stable period of 1988, between \"\n","    \"memory and the present. Anthony Hopkins' excellent performance is certainly the core of the film, and it is \"\n","    \"also rare to see James Hawes, who is making his feature film debut, making his debut with a rather skilled \"\n","    \"and restrained approach.\"\n",")"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:08:46.647608Z","iopub.status.busy":"2024-04-06T05:08:46.647020Z","iopub.status.idle":"2024-04-06T05:08:46.653779Z","shell.execute_reply":"2024-04-06T05:08:46.652729Z","shell.execute_reply.started":"2024-04-06T05:08:46.647563Z"},"trusted":true},"outputs":[],"source":["# Case 5\n","text = (\n","    \"One of the most puzzling aspects of \\\"Goldfinger\\\" is the lack of insight and personal perspective from the \"\n","    \"director regarding the sensational \\\"Jia Ning case\\\" and its mastermind. The character Cheng Yiyuan, portrayed \"\n","    \"by Tony Leung Chiu-Wai, becomes arrogant and driven by greed after entering the rapidly growing financial \"\n","    \"market, resorting to increasingly ruthless methods. The film employs visual and stylistic techniques to \"\n","    \"emphasize the exaggerated sense of decadence and corruption.\\n\\n\"\n","    \"However, beneath the surface dazzle, it merely reinforces the conventional impression of powerful and wealthy \"\n","    \"\\\"villains.\\\" Several scenes in the film's composition clearly bear the influence of both Martin Scorsese's \"\n","    \"earlier and recent works, which is unnecessary. Chapman To's mocking reference to Joe Pesci's \\\"funny how\\\" \"\n","    \"sequence in \\\"Goodfellas\\\" (1990) invites the audience to make comparisons but comes off as awkward.\\n\\n\"\n","    \"I have read some interviews with Felix Chong and Tony Leung Chiu-Wai, where Chong discusses his childhood \"\n","    \"memories of observing how the Jia Ning case affected people in the community, and Leung casually mentions that \"\n","    \"the Jia Ning chairman was probably just an ordinary businessman initially driven by a desire to make money. \"\n","    \"These aspects are more thought-provoking than what \\\"Goldfinger\\\" presents. The current script focuses on the \"\n","    \"confrontation between Cheng Yiyuan and the anti-corruption agency, skillfully maintaining a high-intensity \"\n","    \"dramatic tension. However, it falls short in exploring the significant aspects of real-life events. It treats \"\n","    \"past history merely as a blueprint for genre films, stopping at technical craftsmanship and thrilling plot \"\n","    \"development. This falls short of what can be considered a truly exemplary Hong Kong story.\"\n",")"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:08:48.606109Z","iopub.status.busy":"2024-04-06T05:08:48.605317Z","iopub.status.idle":"2024-04-06T05:08:48.635531Z","shell.execute_reply":"2024-04-06T05:08:48.634582Z","shell.execute_reply.started":"2024-04-06T05:08:48.606074Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1, tensor([[0.2702, 0.7298]], device='cuda:0'))"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(text, model, tokenizer, device)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
